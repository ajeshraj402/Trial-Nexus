{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c35de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.9 (tags/v3.13.9:8183fa5, Oct 14 2025, 14:09:13) [MSC v.1944 64 bit (AMD64)]\n",
      "Platform: Windows-11-10.0.26200-SP0\n",
      "\n",
      "Loaded dataset: clinicaltrials/2021/trec-ct-2022\n",
      "Docs handler  : <ir_datasets.datasets.clinicaltrials.ClinicalTrialsDocs object at 0x0000025E93107ED0>\n",
      "Queries handler: <ir_datasets.formats.trec.TrecXmlQueries object at 0x0000025E93148F30>\n",
      "\n",
      "Has qrels?: False\n",
      "\n",
      "Counts (may take time):\n",
      " - #docs   : 375580\n",
      " - #queries: 50\n",
      "\n",
      "Torch:\n",
      " - torch version: 2.9.1+cpu\n",
      " - cuda available: False\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import ir_datasets\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "DATASET_NAME = \"clinicaltrials/2021/trec-ct-2022\"\n",
    "ds = ir_datasets.load(DATASET_NAME)\n",
    "\n",
    "print(\"\\nLoaded dataset:\", DATASET_NAME)\n",
    "print(\"Docs handler  :\", ds.docs_handler())\n",
    "print(\"Queries handler:\", ds.queries_handler())\n",
    "\n",
    "print(\"\\nHas qrels?:\", ds.has_qrels())\n",
    "\n",
    "# Qrels schema + a few examples\n",
    "if ds.has_qrels():\n",
    "    print(\"Qrels definition:\", ds.qrels_defs())\n",
    "    qrels_preview = []\n",
    "    for i, qrel in enumerate(ds.qrels_iter()):\n",
    "        qrels_preview.append(qrel)\n",
    "        if i >= 4:\n",
    "            break\n",
    "    print(\"First 5 qrels:\", qrels_preview)\n",
    "\n",
    "# Counts (if available)\n",
    "print(\"\\nCounts (may take time):\")\n",
    "print(\" - #docs   :\", ds.docs_count())\n",
    "print(\" - #queries:\", ds.queries_count())\n",
    "if ds.has_qrels():\n",
    "    print(\" - #qrels  :\", ds.qrels_count())\n",
    "\n",
    "print(\"\\nTorch:\")\n",
    "print(\" - torch version:\", torch.__version__)\n",
    "print(\" - cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\" - gpu:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde7d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [starting] https://www.trec-cds.org/topics2022.xml\n",
      "[INFO] [finished] https://www.trec-cds.org/topics2022.xml: [00:00] [32.4kB] [1.70MB/s]\n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries: 50\n",
      "Query object example: GenericQuery(query_id='1', text='\\nA 19-year-old male came to clinic with some sexual concern.  He recently engaged in a relationship and is worried about the satisfaction of his girlfriend. He has a \"baby face\" according to his girlfriend\\'s statement and he is not as muscular as his classmates.  On physical examination, there is some pubic hair and poorly developed secondary sexual characteristics. He is unable to detect coffee smell during the examination, but the visual acuity is normal. Ultrasound reveals the testes volume of 1-2 ml. The hormonal evaluation showed serum testosterone level of 65 ng/dL with low levels of GnRH.\\n')\n",
      "Downloading qrels from: https://trec.nist.gov/data/trials/qrels2022.txt\n",
      "Saved to: C:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\notebooks\\data\\trec2022\\qrels2022.txt\n",
      "Parsed qrels lines: 35394\n",
      "First 5 qrels: [('1', 'NCT00000409', 0), ('1', 'NCT00001148', 0), ('1', 'NCT00001181', 0), ('1', 'NCT00001202', 0), ('1', 'NCT00001270', 0)]\n",
      "\n",
      "#topics with >=1 relevant doc: 50\n",
      "topic 1: #relevant=193\n",
      "topic 2: #relevant=398\n",
      "topic 3: #relevant=79\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "DATASET_NAME = \"clinicaltrials/2021/trec-ct-2022\"\n",
    "ds = ir_datasets.load(DATASET_NAME)\n",
    "\n",
    "# ---- Load queries (topics) ----\n",
    "queries = list(ds.queries_iter())\n",
    "print(\"Loaded queries:\", len(queries))\n",
    "print(\"Query object example:\", queries[0])\n",
    "\n",
    "# ---- Qrels: use built-in if present, otherwise download qrels2022.txt ----\n",
    "qrels = None\n",
    "\n",
    "if ds.has_qrels():\n",
    "    qrels = list(ds.qrels_iter())\n",
    "    print(\"Loaded qrels from ir_datasets:\", len(qrels))\n",
    "else:\n",
    "    # Download qrels2022.txt into your project (tiny file)\n",
    "    qrels_dir = Path(\"data\") / \"trec2022\"\n",
    "    qrels_dir.mkdir(parents=True, exist_ok=True)\n",
    "    qrels_path = qrels_dir / \"qrels2022.txt\"\n",
    "\n",
    "    if not qrels_path.exists():\n",
    "        url = \"https://trec.nist.gov/data/trials/qrels2022.txt\"\n",
    "        print(\"Downloading qrels from:\", url)\n",
    "        urllib.request.urlretrieve(url, qrels_path)\n",
    "        print(\"Saved to:\", qrels_path.resolve())\n",
    "    else:\n",
    "        print(\"Using cached qrels file:\", qrels_path.resolve())\n",
    "\n",
    "    # Parse TREC qrels format: topic 0 doc_id rel\n",
    "    # We'll keep only rel > 0 as relevant\n",
    "    qrels = []\n",
    "    with open(qrels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 4:\n",
    "                continue\n",
    "            topic_id, _, doc_id, rel = parts\n",
    "            qrels.append((topic_id, doc_id, int(rel)))\n",
    "\n",
    "    print(\"Parsed qrels lines:\", len(qrels))\n",
    "    # quick preview\n",
    "    print(\"First 5 qrels:\", qrels[:5])\n",
    "\n",
    "# Build a relevance dict: {topic_id: set(relevant_doc_ids)}\n",
    "qrels_rel = {}\n",
    "for topic_id, doc_id, rel in qrels:\n",
    "    if rel > 0:\n",
    "        qrels_rel.setdefault(str(topic_id), set()).add(str(doc_id))\n",
    "\n",
    "print(\"\\n#topics with >=1 relevant doc:\", len(qrels_rel))\n",
    "# show a couple topics\n",
    "some_topics = list(qrels_rel.keys())[:3]\n",
    "for t in some_topics:\n",
    "    print(f\"topic {t}: #relevant={len(qrels_rel[t])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd2c886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\notebooks\n",
      "\n",
      "Doc type: <class 'ir_datasets.datasets.clinicaltrials.ClinicalTrialsDoc'>\n",
      "Doc id: NCT00000102\n",
      "\n",
      "Fields on doc object (truncated to first 40):\n",
      "['condition', 'count', 'detailed_description', 'doc_id', 'eligibility', 'index', 'summary', 'title']\n",
      "\n",
      "Doc field values (truncated):\n",
      "- condition: \n",
      "- detailed_description: \n",
      "    \n",
      "      This protocol is designed to assess both acute and chronic effects of the calcium channel\n",
      "      antagonist, nifedipine, on the hypothalamic-pituitary-adrenal axis in patients with\n",
      "      congenital adrenal hyperplasia. The multicenter tr ...[truncated]...\n",
      "- doc_id: NCT00000102\n",
      "- eligibility: \n",
      "      \n",
      "        Inclusion Criteria:\n",
      "\n",
      "          -  diagnosed with Congenital Adrenal Hyperplasia (CAH)\n",
      "\n",
      "          -  normal ECG during baseline evaluation\n",
      "\n",
      "        Exclusion Criteria:\n",
      "\n",
      "          -  history of liver disease, or elevated liver f ...[truncated]...\n",
      "- summary: \n",
      "    \n",
      "      This study will test the ability of extended release nifedipine (Procardia XL), a blood\n",
      "      pressure medication, to permit a decrease in the dose of glucocorticoid medication children\n",
      "      take to treat congenital adrenal hyperplasia ...[truncated]...\n",
      "- title: Congenital Adrenal Hyperplasia: Calcium Channels as Therapeutic Targets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import ir_datasets\n",
    "\n",
    "DATASET_NAME = \"clinicaltrials/2021/trec-ct-2022\"\n",
    "ds = ir_datasets.load(DATASET_NAME)\n",
    "\n",
    "# Show current working directory (important for paths)\n",
    "print(\"CWD:\", Path.cwd())\n",
    "\n",
    "# Grab one doc example\n",
    "doc = next(ds.docs_iter())\n",
    "print(\"\\nDoc type:\", type(doc))\n",
    "print(\"Doc id:\", getattr(doc, \"doc_id\", None))\n",
    "\n",
    "# Print all available fields on this doc object (non-private)\n",
    "fields = [k for k in dir(doc) if not k.startswith(\"_\")]\n",
    "print(\"\\nFields on doc object (truncated to first 40):\")\n",
    "print(fields[:40])\n",
    "\n",
    "# Print a readable dict of field->value (truncate long strings)\n",
    "def trunc(x, n=250):\n",
    "    s = str(x)\n",
    "    return s if len(s) <= n else s[:n] + \" ...[truncated]...\"\n",
    "\n",
    "print(\"\\nDoc field values (truncated):\")\n",
    "for k in fields:\n",
    "    v = getattr(doc, k)\n",
    "    # skip methods\n",
    "    if callable(v):\n",
    "        continue\n",
    "    print(f\"- {k}: {trunc(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d7399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading docs (sample): 100%|██████████| 5000/5000 [00:00<00:00, 6147.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample built:\n",
      " - docs: 5000\n",
      " - example doc_id: NCT00000102\n",
      " - example token count: 202\n",
      " - first 30 tokens: ['congenital', 'adrenal', 'hyperplasia', 'calcium', 'channels', 'as', 'therapeutic', 'targets', 'this', 'study', 'will', 'test', 'the', 'ability', 'of', 'extended', 'release', 'nifedipine', 'procardia', 'xl', 'a', 'blood', 'pressure', 'medication', 'to', 'permit', 'a', 'decrease', 'in', 'the']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import ir_datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATASET_NAME = \"clinicaltrials/2021/trec-ct-2022\"\n",
    "ds = ir_datasets.load(DATASET_NAME)\n",
    "\n",
    "# --- Fix paths: set project root as parent of notebooks/ ---\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "\n",
    "# Where we will save artifacts later\n",
    "BM25_INDEX_DIR = PROJECT_ROOT / \"models\" / \"non_user\" / \"bm25\" / \"index\"\n",
    "BM25_RUNS_DIR  = PROJECT_ROOT / \"models\" / \"non_user\" / \"bm25\" / \"runs\"\n",
    "BM25_INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BM25_RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Basic normalization/tokenizer ---\n",
    "_ws = re.compile(r\"\\s+\")\n",
    "_tok = re.compile(r\"[A-Za-z0-9]+\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    s = _ws.sub(\" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return _tok.findall(s.lower())\n",
    "\n",
    "def doc_to_text(doc) -> str:\n",
    "    parts = [\n",
    "        getattr(doc, \"title\", \"\"),\n",
    "        getattr(doc, \"summary\", \"\"),\n",
    "        getattr(doc, \"detailed_description\", \"\"),\n",
    "        getattr(doc, \"eligibility\", \"\"),\n",
    "        getattr(doc, \"condition\", \"\"),\n",
    "    ]\n",
    "    return normalize_text(\" \".join(p for p in parts if p))\n",
    "\n",
    "# --- Build a SMALL sample corpus (first N docs) to validate ---\n",
    "N_SAMPLE = 5000\n",
    "\n",
    "doc_ids = []\n",
    "tokenized_corpus = []\n",
    "\n",
    "for i, doc in enumerate(tqdm(ds.docs_iter(), total=N_SAMPLE, desc=\"Reading docs (sample)\")):\n",
    "    if i >= N_SAMPLE:\n",
    "        break\n",
    "    doc_ids.append(doc.doc_id)\n",
    "    tokenized_corpus.append(tokenize(doc_to_text(doc)))\n",
    "\n",
    "print(\"\\nSample built:\")\n",
    "print(\" - docs:\", len(doc_ids))\n",
    "print(\" - example doc_id:\", doc_ids[0])\n",
    "print(\" - example token count:\", len(tokenized_corpus[0]))\n",
    "print(\" - first 30 tokens:\", tokenized_corpus[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b95d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOPIC: 1\n",
      "QUERY (first 220 chars): a 19-year-old male came to clinic with some sexual concern. he recently engaged in a relationship and is worried about the satisfaction of his girlfriend. he has a \"baby face\" according to his girlfriend's statement and  ...\n",
      "\n",
      "Top-10 doc_ids + scores:\n",
      " 1. NCT00005669   score=165.3529\n",
      " 2. NCT00000683   score=164.2495\n",
      " 3. NCT00001202   score=163.4320\n",
      " 4. NCT00000383   score=160.9882\n",
      " 5. NCT00001181   score=159.1987\n",
      " 6. NCT00001763   score=158.1683\n",
      " 7. NCT00000387   score=157.2092\n",
      " 8. NCT00005664   score=154.5284\n",
      " 9. NCT00001412   score=154.3683\n",
      "10. NCT00005784   score=154.1880\n",
      "\n",
      "================================================================================\n",
      "TOPIC: 2\n",
      "QUERY (first 220 chars): a 32-year-old woman comes to the hospital with vaginal spotting. her last menstrual period was 10 weeks ago. she has regular menses lasting for 6 days and repeating every 29 days. medical history is significant for appen ...\n",
      "\n",
      "Top-10 doc_ids + scores:\n",
      " 1. NCT00000862   score=139.9716\n",
      " 2. NCT00005669   score=129.7251\n",
      " 3. NCT00000683   score=129.5634\n",
      " 4. NCT00001454   score=129.2484\n",
      " 5. NCT00003702   score=129.0841\n",
      " 6. NCT00001848   score=127.4566\n",
      " 7. NCT00005009   score=126.3328\n",
      " 8. NCT00000927   score=125.9127\n",
      " 9. NCT00001481   score=123.0891\n",
      "10. NCT00005064   score=122.9948\n",
      "\n",
      "================================================================================\n",
      "TOPIC: 3\n",
      "QUERY (first 220 chars): a 51-year-old man comes to the office complaining of fatigue and some sexual problems including lack of libido. the patient doesn't smoke or use any illicit drug. blood pressure is 120/80 mm hg and pulse is 70/min. oxyge ...\n",
      "\n",
      "Top-10 doc_ids + scores:\n",
      " 1. NCT00005541   score=177.6435\n",
      " 2. NCT00001596   score=143.8493\n",
      " 3. NCT00001455   score=142.3446\n",
      " 4. NCT00003966   score=140.8476\n",
      " 5. NCT00005559   score=136.7116\n",
      " 6. NCT00001941   score=135.5709\n",
      " 7. NCT00001541   score=134.0731\n",
      " 8. NCT00001910   score=132.2111\n",
      " 9. NCT00004734   score=131.1095\n",
      "10. NCT00001399   score=130.6638\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import ir_datasets\n",
    "\n",
    "# Build BM25 on the sample tokenized corpus we created in Cell 4\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Load queries again (topics)\n",
    "ds = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2022\")\n",
    "queries = list(ds.queries_iter())\n",
    "\n",
    "def bm25_retrieve(query_text: str, topk: int = 10):\n",
    "    q_tokens = tokenize(normalize_text(query_text))\n",
    "    scores = bm25.get_scores(q_tokens)  # numpy array\n",
    "    top_idx = np.argsort(scores)[::-1][:topk]\n",
    "    results = [(doc_ids[i], float(scores[i])) for i in top_idx]\n",
    "    return results\n",
    "\n",
    "# Try 3 queries (1, 2, 3) against the 5k-doc sample\n",
    "for q in queries[:3]:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOPIC:\", q.query_id)\n",
    "    print(\"QUERY (first 220 chars):\", normalize_text(q.text)[:220], \"...\")\n",
    "\n",
    "    hits = bm25_retrieve(q.text, topk=10)\n",
    "    print(\"\\nTop-10 doc_ids + scores:\")\n",
    "    for rank, (did, sc) in enumerate(hits, start=1):\n",
    "        print(f\"{rank:2d}. {did}   score={sc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e67d6dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation (BM25 on 5k-doc sample):\n",
      "Topics evaluated: 50  /  50\n",
      "\n",
      "K=10\n",
      " Precision@k: 0.084\n",
      " Recall@k   : 0.0102\n",
      "\n",
      "K=100\n",
      " Precision@k: 0.0122\n",
      " Recall@k   : 0.0139\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import ir_datasets\n",
    "import numpy as np\n",
    "\n",
    "# Load queries\n",
    "ds = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2022\")\n",
    "queries = list(ds.queries_iter())\n",
    "\n",
    "# If qrels_rel is not in memory (restart), re-load it from the file we downloaded:\n",
    "# (If qrels_rel already exists from Cell 2, this won't hurt)\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "qrels_path = PROJECT_ROOT / \"data\" / \"trec2022\" / \"qrels2022.txt\"\n",
    "if not qrels_path.exists():\n",
    "    # fallback: if your file ended up under notebooks/data\n",
    "    qrels_path = Path.cwd() / \"data\" / \"trec2022\" / \"qrels2022.txt\"\n",
    "\n",
    "qrels_rel = {}\n",
    "with open(qrels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 4:\n",
    "            continue\n",
    "        topic_id, _, doc_id, rel = parts\n",
    "        rel = int(rel)\n",
    "        if rel > 0:\n",
    "            qrels_rel.setdefault(str(topic_id), set()).add(str(doc_id))\n",
    "\n",
    "def retrieve_topk_for_query(qtext: str, k: int):\n",
    "    q_tokens = tokenize(normalize_text(qtext))\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    top_idx = np.argsort(scores)[::-1][:k]\n",
    "    return [doc_ids[i] for i in top_idx]\n",
    "\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    hits = sum(1 for d in retrieved_k if d in relevant)\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    retrieved_k = set(retrieved[:k])\n",
    "    hits = len(retrieved_k.intersection(relevant))\n",
    "    return hits / len(relevant)\n",
    "\n",
    "# Evaluate across all 50 topics\n",
    "Ks = [10, 100]\n",
    "agg = {k: {\"p\": [], \"r\": []} for k in Ks}\n",
    "\n",
    "missing_topics = 0\n",
    "for q in queries:\n",
    "    tid = str(q.query_id)\n",
    "    relevant = qrels_rel.get(tid, set())\n",
    "    if not relevant:\n",
    "        missing_topics += 1\n",
    "        continue\n",
    "\n",
    "    retrieved_100 = retrieve_topk_for_query(q.text, k=max(Ks))  # get once\n",
    "\n",
    "    for k in Ks:\n",
    "        agg[k][\"p\"].append(precision_at_k(retrieved_100, relevant, k))\n",
    "        agg[k][\"r\"].append(recall_at_k(retrieved_100, relevant, k))\n",
    "\n",
    "print(\"Evaluation (BM25 on 5k-doc sample):\")\n",
    "print(\"Topics evaluated:\", len(queries) - missing_topics, \" / \", len(queries))\n",
    "for k in Ks:\n",
    "    print(f\"\\nK={k}\")\n",
    "    print(\" Precision@k:\", round(float(np.mean(agg[k]['p'])), 4))\n",
    "    print(\" Recall@k   :\", round(float(np.mean(agg[k]['r'])), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "830e8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building full corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375580/375580 [01:23<00:00, 4493.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total docs indexed: 375580\n",
      "BM25 full index built.\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "from tqdm.auto import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "ds = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2022\")\n",
    "\n",
    "doc_ids_full = []\n",
    "tokenized_corpus_full = []\n",
    "\n",
    "print(\"Building full corpus...\")\n",
    "for doc in tqdm(ds.docs_iter(), total=ds.docs_count()):\n",
    "    doc_ids_full.append(doc.doc_id)\n",
    "    tokenized_corpus_full.append(tokenize(doc_to_text(doc)))\n",
    "\n",
    "print(\"\\nTotal docs indexed:\", len(doc_ids_full))\n",
    "\n",
    "bm25_full = BM25Okapi(tokenized_corpus_full)\n",
    "\n",
    "print(\"BM25 full index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8177212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation (BM25 FULL corpus):\n",
      "\n",
      "K=10\n",
      " Precision@k: 0.384\n",
      " Recall@k   : 0.0384\n",
      "\n",
      "K=100\n",
      " Precision@k: 0.1842\n",
      " Recall@k   : 0.1393\n"
     ]
    }
   ],
   "source": [
    "ds = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2022\")\n",
    "queries = list(ds.queries_iter())\n",
    "\n",
    "# Reuse qrels_rel from earlier (reload from file if needed)\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "qrels_path = PROJECT_ROOT / \"data\" / \"trec2022\" / \"qrels2022.txt\"\n",
    "if not qrels_path.exists():\n",
    "    qrels_path = Path.cwd() / \"data\" / \"trec2022\" / \"qrels2022.txt\"\n",
    "\n",
    "qrels_rel = {}\n",
    "with open(qrels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 4:\n",
    "            continue\n",
    "        topic_id, _, doc_id, rel = parts\n",
    "        rel = int(rel)\n",
    "        if rel > 0:\n",
    "            qrels_rel.setdefault(str(topic_id), set()).add(str(doc_id))\n",
    "\n",
    "def retrieve_topk_full(qtext: str, k: int):\n",
    "    q_tokens = tokenize(normalize_text(qtext))\n",
    "    scores = bm25_full.get_scores(q_tokens)\n",
    "    top_idx = np.argsort(scores)[::-1][:k]\n",
    "    return [doc_ids_full[i] for i in top_idx]\n",
    "\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    hits = sum(1 for d in retrieved_k if d in relevant)\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    retrieved_k = set(retrieved[:k])\n",
    "    hits = len(retrieved_k.intersection(relevant))\n",
    "    return hits / len(relevant)\n",
    "\n",
    "Ks = [10, 100]\n",
    "agg = {k: {\"p\": [], \"r\": []} for k in Ks}\n",
    "\n",
    "for q in queries:\n",
    "    tid = str(q.query_id)\n",
    "    relevant = qrels_rel.get(tid, set())\n",
    "    retrieved = retrieve_topk_full(q.text, k=max(Ks))  # compute once\n",
    "    for k in Ks:\n",
    "        agg[k][\"p\"].append(precision_at_k(retrieved, relevant, k))\n",
    "        agg[k][\"r\"].append(recall_at_k(retrieved, relevant, k))\n",
    "\n",
    "print(\"Evaluation (BM25 FULL corpus):\")\n",
    "for k in Ks:\n",
    "    print(f\"\\nK={k}\")\n",
    "    print(\" Precision@k:\", round(float(np.mean(agg[k]['p'])), 4))\n",
    "    print(\" Recall@k   :\", round(float(np.mean(agg[k]['r'])), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c174f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 1: age=19, sex=None\n",
      "TOPIC 2: age=32, sex=None\n",
      "TOPIC 3: age=51, sex=M\n",
      "\n",
      "Parsed constraints for BM25 top-10 of topic 1:\n",
      "NCT00005669 score=165.35  constraints={'min_age': None, 'max_age': None, 'sex': None}  passes=True\n",
      "NCT00000683 score=164.25  constraints={'min_age': None, 'max_age': None, 'sex': None}  passes=True\n",
      "NCT00001202 score=163.43  constraints={'min_age': None, 'max_age': None, 'sex': None}  passes=True\n",
      "NCT00000383 score=160.99  constraints={'min_age': None, 'max_age': None, 'sex': None}  passes=True\n",
      "NCT00001181 score=159.20  constraints={'min_age': None, 'max_age': None, 'sex': None}  passes=True\n",
      "NCT00001763 score=158.17  constraints={'min_age': None, 'max_age': None, 'sex': None}  passes=True\n",
      "NCT00000387 score=157.21  constraints={'min_age': None, 'max_age': None, 'sex': None}  passes=True\n",
      "NCT00005664 score=154.53  constraints={'min_age': None, 'max_age': None, 'sex': 'ALL'}  passes=True\n",
      "NCT00001412 score=154.37  constraints={'min_age': None, 'max_age': None, 'sex': None}  passes=True\n",
      "NCT00005784 score=154.19  constraints={'min_age': None, 'max_age': None, 'sex': None}  passes=True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "# --- Query parsing: extract age/sex from patient narrative ---\n",
    "_age_pat = re.compile(r\"\\b(\\d{1,3})-year-old\\b\", re.IGNORECASE)\n",
    "_male_pat = re.compile(r\"\\b(male|man|boy|gentleman|he|his)\\b\", re.IGNORECASE)\n",
    "_female_pat = re.compile(r\"\\b(female|woman|girl|lady|she|her|pregnan)\\w*\\b\", re.IGNORECASE)\n",
    "\n",
    "def extract_user_age_sex(query_text: str) -> Tuple[Optional[int], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Returns (age, sex) where sex is 'M', 'F', or None.\n",
    "    \"\"\"\n",
    "    text = query_text.strip()\n",
    "    m = _age_pat.search(text)\n",
    "    age = int(m.group(1)) if m else None\n",
    "\n",
    "    # crude but effective heuristic\n",
    "    male = bool(_male_pat.search(text))\n",
    "    female = bool(_female_pat.search(text))\n",
    "\n",
    "    sex = None\n",
    "    if male and not female:\n",
    "        sex = \"M\"\n",
    "    elif female and not male:\n",
    "        sex = \"F\"\n",
    "    # if both or neither -> unknown\n",
    "    return age, sex\n",
    "\n",
    "\n",
    "# --- Eligibility parsing: infer allowed sex and age bounds from eligibility text ---\n",
    "# We'll be conservative: if we can't parse, we won't filter it out.\n",
    "\n",
    "_between_pat = re.compile(r\"\\bbetween\\s+(\\d{1,3})\\s+and\\s+(\\d{1,3})\\s+years?\\b\", re.IGNORECASE)\n",
    "_age_ge_pat = re.compile(r\"\\b(\\d{1,3})\\s+years?\\s+(?:of\\s+age\\s+)?and\\s+older\\b|\\bat\\s+least\\s+(\\d{1,3})\\s+years?\\b\", re.IGNORECASE)\n",
    "_age_le_pat = re.compile(r\"\\b(\\d{1,3})\\s+years?\\s+(?:of\\s+age\\s+)?and\\s+younger\\b|\\bno\\s+more\\s+than\\s+(\\d{1,3})\\s+years?\\b|\\bup\\s+to\\s+(\\d{1,3})\\s+years?\\b\", re.IGNORECASE)\n",
    "\n",
    "_males_only = re.compile(r\"\\b(males?\\s+only|men\\s+only|male\\s+subjects?\\s+only)\\b\", re.IGNORECASE)\n",
    "_females_only = re.compile(r\"\\b(females?\\s+only|women\\s+only|female\\s+subjects?\\s+only)\\b\", re.IGNORECASE)\n",
    "_males = re.compile(r\"\\b(males?|men)\\b\", re.IGNORECASE)\n",
    "_females = re.compile(r\"\\b(females?|women)\\b\", re.IGNORECASE)\n",
    "\n",
    "def parse_eligibility_constraints(elig_text: str) -> Dict[str, Optional[object]]:\n",
    "    \"\"\"\n",
    "    Returns dict: {'min_age': int|None, 'max_age': int|None, 'sex': 'M'|'F'|'ALL'|None}\n",
    "    sex meaning:\n",
    "      - 'M'   => males only\n",
    "      - 'F'   => females only\n",
    "      - 'ALL' => both sexes allowed\n",
    "      - None  => unknown / can't infer\n",
    "    \"\"\"\n",
    "    if not elig_text:\n",
    "        return {\"min_age\": None, \"max_age\": None, \"sex\": None}\n",
    "\n",
    "    t = \" \".join(str(elig_text).split())\n",
    "\n",
    "    # Sex parsing\n",
    "    sex = None\n",
    "    if _males_only.search(t):\n",
    "        sex = \"M\"\n",
    "    elif _females_only.search(t):\n",
    "        sex = \"F\"\n",
    "    else:\n",
    "        has_m = bool(_males.search(t))\n",
    "        has_f = bool(_females.search(t))\n",
    "        # if both words appear, likely both allowed (not always, but okay baseline)\n",
    "        if has_m and has_f:\n",
    "            sex = \"ALL\"\n",
    "        elif has_m and not has_f:\n",
    "            # could be male-only or just mentions men; keep unknown unless explicit\n",
    "            sex = None\n",
    "        elif has_f and not has_m:\n",
    "            sex = None\n",
    "\n",
    "    # Age parsing\n",
    "    min_age = None\n",
    "    max_age = None\n",
    "\n",
    "    m = _between_pat.search(t)\n",
    "    if m:\n",
    "        min_age = int(m.group(1))\n",
    "        max_age = int(m.group(2))\n",
    "\n",
    "    m = _age_ge_pat.search(t)\n",
    "    if m:\n",
    "        # pattern has 2 groups, only one will be non-None\n",
    "        g1 = m.group(1)\n",
    "        g2 = m.group(2)\n",
    "        mn = int(g1 or g2)\n",
    "        min_age = mn if (min_age is None or mn > min_age) else min_age\n",
    "\n",
    "    m = _age_le_pat.search(t)\n",
    "    if m:\n",
    "        # multiple alt groups; pick the first non-None\n",
    "        mx = next((g for g in m.groups() if g is not None), None)\n",
    "        if mx is not None:\n",
    "            mx = int(mx)\n",
    "            max_age = mx if (max_age is None or mx < max_age) else max_age\n",
    "\n",
    "    return {\"min_age\": min_age, \"max_age\": max_age, \"sex\": sex}\n",
    "\n",
    "\n",
    "def passes_hard_filters(user_age: Optional[int], user_sex: Optional[str], constraints: Dict[str, Optional[object]]) -> bool:\n",
    "    \"\"\"\n",
    "    Conservative filter:\n",
    "      - If we can't parse constraint, we don't exclude.\n",
    "      - If user age/sex unknown, don't exclude.\n",
    "    \"\"\"\n",
    "    mn, mx, sx = constraints[\"min_age\"], constraints[\"max_age\"], constraints[\"sex\"]\n",
    "\n",
    "    if user_age is not None:\n",
    "        if mn is not None and user_age < mn:\n",
    "            return False\n",
    "        if mx is not None and user_age > mx:\n",
    "            return False\n",
    "\n",
    "    if user_sex is not None and sx in (\"M\", \"F\"):\n",
    "        if user_sex != sx:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# --- Sanity check on first 3 topics ---\n",
    "for q in queries[:3]:\n",
    "    age, sex = extract_user_age_sex(q.text)\n",
    "    print(f\"TOPIC {q.query_id}: age={age}, sex={sex}\")\n",
    "\n",
    "# --- Sanity check on 5 retrieved docs from topic 1 (from your earlier BM25 top10 list)\n",
    "# We'll fetch those docs and see what constraints we parse.\n",
    "doc_map = {}  # lazy load doc objects we need\n",
    "docs_iter = ds.docs_iter()\n",
    "# Build a quick lookup for a handful of doc_ids (efficient enough for small count)\n",
    "needed = set([did for did, _ in bm25_retrieve(queries[0].text, topk=10)])\n",
    "for doc in ds.docs_iter():\n",
    "    if doc.doc_id in needed:\n",
    "        doc_map[doc.doc_id] = doc\n",
    "        if len(doc_map) == len(needed):\n",
    "            break\n",
    "\n",
    "print(\"\\nParsed constraints for BM25 top-10 of topic 1:\")\n",
    "u_age, u_sex = extract_user_age_sex(queries[0].text)\n",
    "for did, sc in bm25_retrieve(queries[0].text, topk=10):\n",
    "    doc = doc_map.get(did)\n",
    "    c = parse_eligibility_constraints(getattr(doc, \"eligibility\", \"\") if doc else \"\")\n",
    "    ok = passes_hard_filters(u_age, u_sex, c)\n",
    "    print(f\"{did} score={sc:.2f}  constraints={c}  passes={ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf601b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 1: age=19, sex=M\n",
      "TOPIC 2: age=32, sex=M\n",
      "TOPIC 3: age=51, sex=M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building doc_store: 100%|██████████| 375580/375580 [00:04<00:00, 77922.65it/s] \n",
      "Evaluating hard-filtered BM25: 100%|██████████| 50/50 [07:45<00:00,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation (BM25 + hard filters on FULL corpus):\n",
      "\n",
      "K=10\n",
      " Precision@k: 0.378\n",
      " Recall@k   : 0.0372\n",
      "\n",
      "K=100\n",
      " Precision@k: 0.182\n",
      " Recall@k   : 0.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Better query sex detection: prioritize explicit \"male\"/\"female\" ---\n",
    "_explicit_male = re.compile(r\"\\bmale\\b\", re.IGNORECASE)\n",
    "_explicit_female = re.compile(r\"\\bfemale\\b\", re.IGNORECASE)\n",
    "\n",
    "def extract_user_age_sex(query_text: str):\n",
    "    text = query_text.strip()\n",
    "\n",
    "    # age\n",
    "    m = re.search(r\"\\b(\\d{1,3})-year-old\\b\", text, flags=re.IGNORECASE)\n",
    "    age = int(m.group(1)) if m else None\n",
    "\n",
    "    # sex (explicit first)\n",
    "    if _explicit_male.search(text) and not _explicit_female.search(text):\n",
    "        sex = \"M\"\n",
    "    elif _explicit_female.search(text) and not _explicit_male.search(text):\n",
    "        sex = \"F\"\n",
    "    else:\n",
    "        # fallback pronoun heuristic\n",
    "        male = bool(re.search(r\"\\b(he|his|man|boy|gentleman)\\b\", text, flags=re.IGNORECASE))\n",
    "        female = bool(re.search(r\"\\b(she|her|woman|girl|lady|pregnan)\\w*\\b\", text, flags=re.IGNORECASE))\n",
    "        if male and not female:\n",
    "            sex = \"M\"\n",
    "        elif female and not male:\n",
    "            sex = \"F\"\n",
    "        else:\n",
    "            sex = None\n",
    "\n",
    "    return age, sex\n",
    "\n",
    "# quick sanity check again\n",
    "for q in queries[:3]:\n",
    "    age, sex = extract_user_age_sex(q.text)\n",
    "    print(f\"TOPIC {q.query_id}: age={age}, sex={sex}\")\n",
    "\n",
    "# --- Hard-filtered retrieval over BM25 full index ---\n",
    "K_CANDIDATES = 200  # retrieve from BM25\n",
    "K_EVALS = [10, 100]\n",
    "\n",
    "def bm25_retrieve_full(query_text: str, topk: int):\n",
    "    q_tokens = tokenize(normalize_text(query_text))\n",
    "    scores = bm25_full.get_scores(q_tokens)\n",
    "    top_idx = np.argsort(scores)[::-1][:topk]\n",
    "    return [doc_ids_full[i] for i in top_idx]\n",
    "\n",
    "def filtered_retrieve(query_text: str, user_age, user_sex, topk_bm25=200, max_keep=100):\n",
    "    \"\"\"\n",
    "    Retrieve topk_bm25 by BM25, then filter by eligibility constraints.\n",
    "    Keep up to max_keep after filtering (rank order preserved).\n",
    "    \"\"\"\n",
    "    cand_ids = bm25_retrieve_full(query_text, topk_bm25)\n",
    "\n",
    "    kept = []\n",
    "    for did in cand_ids:\n",
    "        doc = doc_store.get(did)\n",
    "        if doc is None:\n",
    "            continue\n",
    "        constraints = parse_eligibility_constraints(getattr(doc, \"eligibility\", \"\"))\n",
    "        if passes_hard_filters(user_age, user_sex, constraints):\n",
    "            kept.append(did)\n",
    "        if len(kept) >= max_keep:\n",
    "            break\n",
    "    return kept\n",
    "\n",
    "# --- Build a doc_store for fast access (id -> doc object) ---\n",
    "# This is a bit heavy but manageable; we need doc text anyway for BioBERT next.\n",
    "doc_store = {}\n",
    "for doc in tqdm(ds.docs_iter(), total=ds.docs_count(), desc=\"Building doc_store\"):\n",
    "    doc_store[doc.doc_id] = doc\n",
    "\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    hits = sum(1 for d in retrieved[:k] if d in relevant)\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    hits = len(set(retrieved[:k]).intersection(relevant))\n",
    "    return hits / len(relevant)\n",
    "\n",
    "# Evaluate hard-filtered BM25\n",
    "agg = {k: {\"p\": [], \"r\": []} for k in K_EVALS}\n",
    "\n",
    "for q in tqdm(queries, desc=\"Evaluating hard-filtered BM25\"):\n",
    "    tid = str(q.query_id)\n",
    "    relevant = qrels_rel.get(tid, set())\n",
    "\n",
    "    user_age, user_sex = extract_user_age_sex(q.text)\n",
    "    retrieved = filtered_retrieve(q.text, user_age, user_sex, topk_bm25=K_CANDIDATES, max_keep=max(K_EVALS))\n",
    "\n",
    "    # If filter is too strict and returns few docs, pad with unfiltered BM25 so metrics are comparable\n",
    "    if len(retrieved) < max(K_EVALS):\n",
    "        pad = bm25_retrieve_full(q.text, topk=max(K_EVALS))\n",
    "        seen = set(retrieved)\n",
    "        for d in pad:\n",
    "            if d not in seen:\n",
    "                retrieved.append(d)\n",
    "                seen.add(d)\n",
    "            if len(retrieved) >= max(K_EVALS):\n",
    "                break\n",
    "\n",
    "    for k in K_EVALS:\n",
    "        agg[k][\"p\"].append(precision_at_k(retrieved, relevant, k))\n",
    "        agg[k][\"r\"].append(recall_at_k(retrieved, relevant, k))\n",
    "\n",
    "print(\"\\nEvaluation (BM25 + hard filters on FULL corpus):\")\n",
    "for k in K_EVALS:\n",
    "    print(f\"\\nK={k}\")\n",
    "    print(\" Precision@k:\", round(float(np.mean(agg[k]['p'])), 4))\n",
    "    print(\" Recall@k   :\", round(float(np.mean(agg[k]['r'])), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545aa605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\n",
      "BM25_INDEX_DIR: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\models\\non_user\\bm25\\index\n",
      "BM25_RUNS_DIR : c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\models\\non_user\\bm25\\runs\n",
      "EVAL_DIR      : c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\tests\\outputs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "BM25_INDEX_DIR = PROJECT_ROOT / \"models\" / \"non_user\" / \"bm25\" / \"index\"\n",
    "BM25_RUNS_DIR  = PROJECT_ROOT / \"models\" / \"non_user\" / \"bm25\" / \"runs\"\n",
    "EVAL_DIR       = PROJECT_ROOT / \"tests\" / \"outputs\"\n",
    "\n",
    "BM25_INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BM25_RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"BM25_INDEX_DIR:\", BM25_INDEX_DIR)\n",
    "print(\"BM25_RUNS_DIR :\", BM25_RUNS_DIR)\n",
    "print(\"EVAL_DIR      :\", EVAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae38aab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 index saved to: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\models\\non_user\\bm25\\index\\bm25_full.pkl\n",
      "Total docs saved: 375580\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "bm25_path = BM25_INDEX_DIR / \"bm25_full.pkl\"\n",
    "\n",
    "with open(bm25_path, \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"bm25\": bm25_full,\n",
    "            \"doc_ids\": doc_ids_full\n",
    "        },\n",
    "        f,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "\n",
    "print(\"BM25 index saved to:\", bm25_path)\n",
    "print(\"Total docs saved:\", len(doc_ids_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d0185dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 run file saved to: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\models\\non_user\\bm25\\runs\\bm25_trec2022_top100.run\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import numpy as np\n",
    "\n",
    "ds = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2022\")\n",
    "queries = list(ds.queries_iter())\n",
    "\n",
    "RUN_K = 100\n",
    "run_path = BM25_RUNS_DIR / \"bm25_trec2022_top100.run\"\n",
    "\n",
    "with open(run_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for q in queries:\n",
    "        qid = q.query_id\n",
    "        q_tokens = tokenize(normalize_text(q.text))\n",
    "        scores = bm25_full.get_scores(q_tokens)\n",
    "        top_idx = np.argsort(scores)[::-1][:RUN_K]\n",
    "\n",
    "        for rank, i in enumerate(top_idx, start=1):\n",
    "            doc_id = doc_ids_full[i]\n",
    "            score = float(scores[i])\n",
    "            # TREC format: topic Q0 doc rank score tag\n",
    "            f.write(f\"{qid} Q0 {doc_id} {rank} {score:.6f} BM25\\n\")\n",
    "\n",
    "print(\"BM25 run file saved to:\", run_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c0a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\tests\\outputs\\bm25_trec2022_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Precision@10</td>\n",
       "      <td>0.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall@10</td>\n",
       "      <td>0.037235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision@100</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recall@100</td>\n",
       "      <td>0.136989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric     value\n",
       "0   Precision@10  0.378000\n",
       "1      Recall@10  0.037235\n",
       "2  Precision@100  0.182000\n",
       "3     Recall@100  0.136989"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [\n",
    "    {\"metric\": \"Precision@10\",  \"value\": np.mean(agg[10][\"p\"])},\n",
    "    {\"metric\": \"Recall@10\",     \"value\": np.mean(agg[10][\"r\"])},\n",
    "    {\"metric\": \"Precision@100\", \"value\": np.mean(agg[100][\"p\"])},\n",
    "    {\"metric\": \"Recall@100\",    \"value\": np.mean(agg[100][\"r\"])},\n",
    "]\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "metrics_path = EVAL_DIR / \"bm25_trec2022_metrics.csv\"\n",
    "df_metrics.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(\"Metrics saved to:\", metrics_path)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "257bc8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config saved to: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\tests\\outputs\\bm25_trec2022_config.json\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"dataset\": \"clinicaltrials/2021/trec-ct-2022\",\n",
    "    \"retriever\": \"BM25\",\n",
    "    \"doc_fields\": [\n",
    "        \"title\",\n",
    "        \"summary\",\n",
    "        \"detailed_description\",\n",
    "        \"eligibility\",\n",
    "        \"condition\"\n",
    "    ],\n",
    "    \"bm25_k_eval\": [10, 100],\n",
    "    \"notes\": \"Baseline non-user retrieval model\"\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "config_path = EVAL_DIR / \"bm25_trec2022_config.json\"\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Config saved to:\", config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccbf9f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building doc_text_map: 100%|██████████| 375580/375580 [00:45<00:00, 8282.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc text map built: 375580\n",
      "Sample doc text length: 1367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Parameters\n",
    "BM25_CANDIDATES = 100   # rerank top-100\n",
    "BIOBERT_TOPK = 10       # final top-k after rerank\n",
    "\n",
    "# Helper: get BM25 candidates\n",
    "def bm25_candidates(query_text, topk=BM25_CANDIDATES):\n",
    "    q_tokens = tokenize(normalize_text(query_text))\n",
    "    scores = bm25_full.get_scores(q_tokens)\n",
    "    idx = np.argsort(scores)[::-1][:topk]\n",
    "    return [(doc_ids_full[i], float(scores[i])) for i in idx]\n",
    "\n",
    "# Build text lookup once (doc_id -> full text)\n",
    "doc_text_map = {}\n",
    "\n",
    "for doc in tqdm(ds.docs_iter(), total=ds.docs_count(), desc=\"Building doc_text_map\"):\n",
    "    doc_text_map[doc.doc_id] = doc_to_text(doc)\n",
    "\n",
    "print(\"Doc text map built:\", len(doc_text_map))\n",
    "print(\"Sample doc text length:\", len(next(iter(doc_text_map.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa7e736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.1+cpu\n",
      "CUDA available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedder: pritamdeka/S-BioBERT-snli-multinli-stsb\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "EMBED_MODEL = \"pritamdeka/S-BioBERT-snli-multinli-stsb\"\n",
    "\n",
    "embedder = SentenceTransformer(\n",
    "    EMBED_MODEL,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Loaded embedder:\", EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bd8d9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioBERT reranked top-10 for topic 1\n",
      " 1. NCT03459326  score=0.4864\n",
      " 2. NCT02014584  score=0.3576\n",
      " 3. NCT04630275  score=0.3340\n",
      " 4. NCT03149692  score=0.3213\n",
      " 5. NCT00494208  score=0.3102\n",
      " 6. NCT02777242  score=0.3043\n",
      " 7. NCT00194636  score=0.3036\n",
      " 8. NCT04049331  score=0.3001\n",
      " 9. NCT01689896  score=0.2760\n",
      "10. NCT00644163  score=0.2648\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def biobert_rerank(query_text, bm25_hits, topk=BIOBERT_TOPK):\n",
    "    \"\"\"\n",
    "    bm25_hits: list of (doc_id, bm25_score)\n",
    "    \"\"\"\n",
    "    docs = [doc_text_map[did] for did, _ in bm25_hits]\n",
    "\n",
    "    # Encode query + docs\n",
    "    q_emb = embedder.encode([query_text], convert_to_numpy=True)\n",
    "    d_emb = embedder.encode(docs, convert_to_numpy=True, show_progress_bar=False)\n",
    "\n",
    "    # Cosine similarity\n",
    "    sims = cosine_similarity(q_emb, d_emb)[0]\n",
    "\n",
    "    # Rerank\n",
    "    idx = np.argsort(sims)[::-1][:topk]\n",
    "    reranked = [(bm25_hits[i][0], float(sims[i])) for i in idx]\n",
    "\n",
    "    return reranked\n",
    "\n",
    "# Quick sanity check on first topic\n",
    "q0 = queries[0]\n",
    "bm25_hits = bm25_candidates(q0.text)\n",
    "reranked = biobert_rerank(q0.text, bm25_hits)\n",
    "\n",
    "print(\"BioBERT reranked top-10 for topic\", q0.query_id)\n",
    "for i, (did, score) in enumerate(reranked, 1):\n",
    "    print(f\"{i:2d}. {did}  score={score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7c15a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BioBERT rerank: 100%|██████████| 50/50 [11:33<00:00, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation (BM25 + BioBERT rerank):\n",
      "\n",
      "K=10\n",
      " Precision@k: 0.31\n",
      " Recall@k   : 0.0266\n",
      "\n",
      "K=100\n",
      " Precision@k: 0.1842\n",
      " Recall@k   : 0.1393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "K_EVAL = [10, 100]\n",
    "agg_biobert = {k: {\"p\": [], \"r\": []} for k in K_EVAL}\n",
    "\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    return sum(1 for d in retrieved[:k] if d in relevant) / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return len(set(retrieved[:k]).intersection(relevant)) / len(relevant)\n",
    "\n",
    "for q in tqdm(queries, desc=\"Evaluating BioBERT rerank\"):\n",
    "    tid = str(q.query_id)\n",
    "    relevant = qrels_rel.get(tid, set())\n",
    "\n",
    "    bm25_hits = bm25_candidates(q.text)\n",
    "    reranked = biobert_rerank(q.text, bm25_hits, topk=max(K_EVAL))\n",
    "    retrieved_ids = [d for d, _ in reranked]\n",
    "\n",
    "    for k in K_EVAL:\n",
    "        agg_biobert[k][\"p\"].append(precision_at_k(retrieved_ids, relevant, k))\n",
    "        agg_biobert[k][\"r\"].append(recall_at_k(retrieved_ids, relevant, k))\n",
    "\n",
    "print(\"\\nEvaluation (BM25 + BioBERT rerank):\")\n",
    "for k in K_EVAL:\n",
    "    print(f\"\\nK={k}\")\n",
    "    print(\" Precision@k:\", round(float(np.mean(agg_biobert[k]['p'])), 4))\n",
    "    print(\" Recall@k   :\", round(float(np.mean(agg_biobert[k]['r'])), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "066764db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing BioBERT run: 100%|██████████| 50/50 [24:19<00:00, 29.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved BioBERT run file to: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\models\\non_user\\biobert\\reranked_runs\\biobert_rerank_trec2022_top100.run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "BIO_RUN_DIR = PROJECT_ROOT / \"models\" / \"non_user\" / \"biobert\" / \"reranked_runs\"\n",
    "BIO_RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_path = BIO_RUN_DIR / \"biobert_rerank_trec2022_top100.run\"\n",
    "\n",
    "RUN_K = 100\n",
    "\n",
    "with open(run_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for q in tqdm(queries, desc=\"Writing BioBERT run\"):\n",
    "        qid = q.query_id\n",
    "        bm25_hits = bm25_candidates(q.text, topk=RUN_K)  # (doc_id, bm25_score)\n",
    "        reranked = biobert_rerank(q.text, bm25_hits, topk=RUN_K)  # (doc_id, biobert_score)\n",
    "\n",
    "        for rank, (doc_id, score) in enumerate(reranked, start=1):\n",
    "            f.write(f\"{qid} Q0 {doc_id} {rank} {score:.6f} BioBERT_biencoder\\n\")\n",
    "\n",
    "print(\"Saved BioBERT run file to:\", run_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb449dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved BioBERT metrics to: c:\\Ajesh_Drive\\PersonalProjects\\ClinicalTrialNexus\\tests\\outputs\\biobert_rerank_trec2022_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Precision@10</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall@10</td>\n",
       "      <td>0.026642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision@100</td>\n",
       "      <td>0.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recall@100</td>\n",
       "      <td>0.139303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric     value\n",
       "0   Precision@10  0.310000\n",
       "1      Recall@10  0.026642\n",
       "2  Precision@100  0.184200\n",
       "3     Recall@100  0.139303"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "EVAL_DIR = PROJECT_ROOT / \"tests\" / \"outputs\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics = [\n",
    "    {\"metric\": \"Precision@10\",  \"value\": float(np.mean(agg_biobert[10][\"p\"]))},\n",
    "    {\"metric\": \"Recall@10\",     \"value\": float(np.mean(agg_biobert[10][\"r\"]))},\n",
    "    {\"metric\": \"Precision@100\", \"value\": float(np.mean(agg_biobert[100][\"p\"]))},\n",
    "    {\"metric\": \"Recall@100\",    \"value\": float(np.mean(agg_biobert[100][\"r\"]))},\n",
    "]\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "metrics_path = EVAL_DIR / \"biobert_rerank_trec2022_metrics.csv\"\n",
    "df_metrics.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(\"Saved BioBERT metrics to:\", metrics_path)\n",
    "df_metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
